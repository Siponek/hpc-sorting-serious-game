%----------------------------------------------------------------------------------------
% Appendix D: Study Materials
%----------------------------------------------------------------------------------------

\section{Introduction}
\label{app:d:introduction}

This appendix provides educational materials to help students and instructors connect the HPC Sorting Serious Game to formal parallel computing concepts. It includes learning objectives, discussion questions, exercises, and connections to OpenMP and MPI programming.

%----------------------------------------------------------------------------------------
\section{Learning Objectives}
\label{app:d:learning-objectives}

\subsection{Foundational Concepts}

After playing the game, students should be able to:

\begin{enumerate}
    \item \textbf{Define Parallelism}: Explain what it means to perform multiple tasks simultaneously
    \item \textbf{Identify Parallel Paradigms}: Distinguish between shared-memory and distributed-memory parallelism
    \item \textbf{Understand Speedup}: Describe how parallel execution can reduce total execution time
    \item \textbf{Recognize Communication Costs}: Explain that coordination between parallel workers has overhead
    \item \textbf{Appreciate Load Balancing}: Understand the importance of distributing work evenly
\end{enumerate}

\subsection{OpenMP Concepts (Single-Player Mode)}

Students should understand:

\begin{enumerate}
    \item Threads share a common memory space
    \item Each thread can have private (local) data
    \item Synchronization is implicit or managed by programmer
    \item Work can be divided among threads
    \item Final merge combines results from parallel execution
\end{enumerate}

\subsection{MPI Concepts (Multiplayer Mode)}

Students should understand:

\begin{enumerate}
    \item Processes have separate memory spaces
    \item Communication requires explicit message passing
    \item Data must be distributed across processes
    \item Communication has latency costs
    \item Coordination patterns (master-worker, collective operations)
\end{enumerate}

%----------------------------------------------------------------------------------------
\section{Discussion Questions}
\label{app:d:discussion-questions}

\subsection{Pre-Game Questions}

Use these questions before gameplay to activate prior knowledge:

\begin{enumerate}
    \item Have you ever worked on a group project? How did you divide the work?
    \item What challenges arise when multiple people work on the same task simultaneously?
    \item How is sorting a deck of cards different when one person does it versus multiple people?
    \item Can you think of tasks that benefit from having multiple workers? Which tasks don't?
\end{enumerate}

\subsection{Post-Game Discussion: Single-Player Mode}

After single-player gameplay:

\begin{enumerate}
    \item How did you use the buffer zones? Why were they helpful?
    \item What strategy did you use to sort the cards efficiently?
    \item How would your strategy change with 10 cards versus 200 cards?
    \item If you could have multiple versions of yourself working simultaneously, how would you coordinate?
    \item What parallels can you draw between the buffers and computer memory?
\end{enumerate}

\subsection{Post-Game Discussion: Multiplayer Mode}

After multiplayer gameplay:

\begin{enumerate}
    \item How did you communicate and coordinate with other players?
    \item What happened when two players tried to work on the same cards?
    \item Did you notice delays when passing cards between players? Why might that occur?
    \item How did you divide the work? Was it balanced?
    \item What would happen if one player had many more cards than others?
    \item How is this different from single-player mode?
\end{enumerate}

\subsection{Connecting to HPC Concepts}

Bridge from game to formal concepts:

\begin{enumerate}
    \item How does single-player mode relate to shared-memory parallel programming?
    \item How does multiplayer mode relate to distributed-memory parallel programming?
    \item What real-world computing problems benefit from parallelism?
    \item What are the trade-offs between shared-memory and distributed-memory approaches?
    \item When might communication costs outweigh parallelism benefits?
\end{enumerate}

%----------------------------------------------------------------------------------------
\section{Classroom Activities}
\label{app:d:classroom-activities}

\subsection{Activity 1: Strategy Competition}

\paragraph{Objective:} Compare different sorting strategies and analyze performance.

\paragraph{Instructions:}
\begin{enumerate}
    \item Divide class into teams of 2--3 students
    \item Each team plays the game with the same configuration (e.g., 50 cards)
    \item Teams develop and document their strategy
    \item Compare completion times and move counts
    \item Discuss which strategies were most efficient and why
\end{enumerate}

\paragraph{Discussion:}
\begin{itemize}
    \item Which strategy was fastest? Why?
    \item Which strategy used fewest moves? Are time and moves correlated?
    \item How do efficient sorting algorithms (merge sort, quick sort) relate to the strategies used?
\end{itemize}

%----------------------------------------------------------------------------------------
\subsection{Activity 2: Scalability Experiment}

\paragraph{Objective:} Investigate how performance changes with problem size and parallelism degree.

\paragraph{Instructions:}
\begin{enumerate}
    \item Students play multiplayer mode with different configurations:
          \begin{itemize}
              \item 2 players, 50 cards
              \item 3 players, 50 cards
              \item 4 players, 50 cards
              \item 2 players, 100 cards
              \item 4 players, 100 cards
          \end{itemize}
    \item Record completion times for each configuration
    \item Plot speedup: $\text{Speedup} = \frac{T_1}{T_n}$ where $T_n$ is time with $n$ players
    \item Discuss whether speedup is linear
\end{enumerate}

\paragraph{Analysis Questions:}
\begin{itemize}
    \item Does doubling the number of players halve the time?
    \item What factors prevent perfect linear speedup?
    \item How does this relate to Amdahl's Law?
\end{itemize}

%----------------------------------------------------------------------------------------
\subsection{Activity 3: Communication Overhead}

\paragraph{Objective:} Understand the cost of communication in distributed systems.

\paragraph{Instructions:}
\begin{enumerate}
    \item Play multiplayer mode with different strategies:
          \begin{itemize}
              \item \textbf{Strategy A}: Minimize card passing (each player sorts their cards independently, final merge only)
              \item \textbf{Strategy B}: Frequent card passing (players exchange cards often to balance work)
          \end{itemize}
    \item Record time and number of card exchanges
    \item Compare strategies
\end{enumerate}

\paragraph{Discussion:}
\begin{itemize}
    \item Which strategy was faster?
    \item What is the trade-off between load balancing and communication?
    \item How does network latency (simulated by passing time) affect the optimal strategy?
    \item How does this relate to MPI programming considerations?
\end{itemize}

%----------------------------------------------------------------------------------------
\section{Exercises}
\label{app:d:exercises}

\subsection{Exercise 1: Parallel Algorithm Design}

\paragraph{Task:}

Design a parallel sorting algorithm for $N$ cards with $P$ players.

\paragraph{Questions:}
\begin{enumerate}
    \item How would you distribute the cards among players?
    \item What sorting algorithm would each player use locally?
    \item How would you merge the sorted results?
    \item What is the theoretical time complexity?
    \item What assumptions are you making about communication costs?
\end{enumerate}

\paragraph{Pseudocode:}

Write pseudocode for your algorithm in both OpenMP-style (shared memory) and MPI-style (message passing).

%----------------------------------------------------------------------------------------
\subsection{Exercise 2: Performance Modeling}

\paragraph{Task:}

Model the expected performance of parallel sorting.

\paragraph{Given:}
\begin{itemize}
    \item Serial sorting time: $T_{\text{serial}} = O(N \log N)$
    \item Number of processes: $P$
    \item Communication time per message: $t_{\text{comm}}$
\end{itemize}

\paragraph{Questions:}
\begin{enumerate}
    \item What is the theoretical parallel time assuming perfect load balance?
    \item How many communication operations are needed?
    \item Express total parallel time including communication overhead
    \item Under what conditions does parallelization provide benefit?
\end{enumerate}

%----------------------------------------------------------------------------------------
\subsection{Exercise 3: OpenMP Implementation}

\paragraph{Task:}

Write OpenMP code to parallelize a sorting algorithm.

\paragraph{Code Template:}

\begin{lstlisting}[language=C++]
#include <omp.h>
#include <algorithm>
#include <vector>

void parallel_sort(std::vector<int>& data, int num_threads) {
    int n = data.size();

    #pragma omp parallel num_threads(num_threads)
    {
        // TODO: Implement parallel sorting
        // 1. Divide data among threads
        // 2. Sort local portion
        // 3. Merge sorted portions
    }
}
\end{lstlisting}

\paragraph{Requirements:}
\begin{itemize}
    \item Use OpenMP directives for parallelization
    \item Each thread sorts a portion of the array
    \item Implement a parallel merge phase
    \item Compare performance with serial sorting
\end{itemize}

%----------------------------------------------------------------------------------------
\subsection{Exercise 4: MPI Implementation}

\paragraph{Task:}

Write MPI code to sort data distributed across processes.

\paragraph{Code Template:}

\begin{lstlisting}[language=C++]
#include <mpi.h>
#include <vector>
#include <algorithm>

void mpi_sort(std::vector<int>& local_data) {
    int rank, size;
    MPI_Comm_rank(MPI_COMM_WORLD, &rank);
    MPI_Comm_size(MPI_COMM_WORLD, &size);

    // TODO: Implement MPI sorting
    // 1. Sort local data
    // 2. Exchange data with other processes
    // 3. Merge to produce globally sorted result
}

int main(int argc, char** argv) {
    MPI_Init(&argc, &argv);

    // Generate or distribute data
    std::vector<int> local_data = get_local_data();

    mpi_sort(local_data);

    MPI_Finalize();
    return 0;
}
\end{lstlisting}

\paragraph{Requirements:}
\begin{itemize}
    \item Use MPI communication primitives (\texttt{MPI\_Send}, \texttt{MPI\_Recv}, \texttt{MPI\_Gather})
    \item Implement sample sort or merge-based approach
    \item Handle data distribution and collection
    \item Measure and report speedup
\end{itemize}

%----------------------------------------------------------------------------------------
\section{Conceptual Mappings}
\label{app:d:conceptual-mappings}

\subsection{Game to OpenMP}

\begin{table}[htbp]
    \centering
    \caption{Detailed game-to-OpenMP concept mapping}
    \begin{tabular}{@{}p{4cm}p{5cm}p{5cm}@{}}
        \toprule
        \textbf{Game Element}         & \textbf{OpenMP Concept}           & \textbf{Code Example}                 \\
        \midrule
        Single-player session         & Parallel region                   & \texttt{\#pragma omp parallel}        \\
        Main card container           & Shared array                      & Global array variable                 \\
        Buffer zones                  & Thread-private storage            & \texttt{private(buffer)}              \\
        No forced turn-taking         & Independent threads               & No explicit synchronization           \\
        Sorting within buffer         & Local computation                 & Serial code in parallel region        \\
        Merging back to main          & Reduction/merge                   & \texttt{\#pragma omp critical}        \\
        Completion check              & Barrier synchronization           & \texttt{\#pragma omp barrier}         \\
        \bottomrule
    \end{tabular}
\end{table}

%----------------------------------------------------------------------------------------
\subsection{Game to MPI}

\begin{table}[htbp]
    \centering
    \caption{Detailed game-to-MPI concept mapping}
    \begin{tabular}{@{}p{4cm}p{5cm}p{5cm}@{}}
        \toprule
        \textbf{Game Element}    & \textbf{MPI Concept}          & \textbf{Code Example}                \\
        \midrule
        Individual player        & MPI process                   & Identified by rank                   \\
        Player's cards           & Local data                    & Local array per process              \\
        Private buffers          & Process-private memory        & Variables not shared                 \\
        Passing cards            & Message passing               & \texttt{MPI\_Send/MPI\_Recv}         \\
        Network delay            & Communication latency         & Inherent in message passing          \\
        Host player              & Master process (rank 0)       & \texttt{if (rank == 0)}              \\
        Final merge              & MPI\_Gather                   & \texttt{MPI\_Gather(...)}            \\
        Completion broadcast     & MPI\_Bcast                    & \texttt{MPI\_Bcast(...)}             \\
        \bottomrule
    \end{tabular}
\end{table}

%----------------------------------------------------------------------------------------
\section{Assessment Rubric}
\label{app:d:assessment-rubric}

For instructors wishing to grade student understanding:

\subsection{Conceptual Understanding (40 points)}

\begin{itemize}
    \item (10 pts) Can identify shared-memory vs. distributed-memory paradigms
    \item (10 pts) Understands role of communication in parallel performance
    \item (10 pts) Can explain speedup and scalability concepts
    \item (10 pts) Recognizes load balancing importance
\end{itemize}

\subsection{Practical Application (30 points)}

\begin{itemize}
    \item (15 pts) Designs reasonable parallel sorting strategy
    \item (15 pts) Implements parallel sorting code (OpenMP or MPI)
\end{itemize}

\subsection{Analysis and Reflection (30 points)}

\begin{itemize}
    \item (10 pts) Analyzes performance data from experiments
    \item (10 pts) Connects game experience to real HPC systems
    \item (10 pts) Reflects on limitations and trade-offs
\end{itemize}

%----------------------------------------------------------------------------------------
\section{Additional Resources}
\label{app:d:additional-resources}

\subsection{Recommended Readings}

\begin{enumerate}
    \item Pacheco, P. (2011). \textit{An Introduction to Parallel Programming}. Morgan Kaufmann.
    \item OpenMP Architecture Review Board. (2021). \textit{OpenMP Application Programming Interface}.
    \item Message Passing Interface Forum. (2021). \textit{MPI: A Message-Passing Interface Standard}.
\end{enumerate}

\subsection{Online Resources}

\begin{itemize}
    \item OpenMP Tutorials: \texttt{https://www.openmp.org/resources/tutorials-articles/}
    \item MPI Tutorial: \texttt{https://mpitutorial.com/}
    \item Parallel Computing Course Materials: \texttt{https://www.cs.cmu.edu/~15418/}
\end{itemize}

\subsection{Hands-On Practice}

\begin{itemize}
    \item Try the game with different configurations
    \item Implement actual parallel sorting code
    \item Profile and optimize parallel programs
    \item Experiment with HPC clusters (if available)
\end{itemize}

%----------------------------------------------------------------------------------------
\section{Instructor Notes}
\label{app:d:instructor-notes}

\subsection{Integration with Curriculum}

\paragraph{As Pre-Lecture Activity (15--20 minutes):}
\begin{itemize}
    \item Students play game before formal HPC lecture
    \item Generates curiosity and questions
    \item Provides concrete reference points for abstract concepts
    \item Follow with discussion connecting gameplay to course material
\end{itemize}

\paragraph{As Lab Exercise (50--90 minutes):}
\begin{itemize}
    \item Structured activities with data collection
    \item Analysis of performance metrics
    \item Written reflection connecting to theory
    \item Implementation of actual parallel code based on strategies
\end{itemize}

\paragraph{As Homework/Assessment:}
\begin{itemize}
    \item Students play independently, record observations
    \item Answer reflection questions
    \item Design parallel algorithms inspired by gameplay
    \item Optional: Implement and benchmark solutions
\end{itemize}

\subsection{Common Student Misconceptions}

Be prepared to address:

\begin{enumerate}
    \item \textbf{"More processes/threads always means faster"}: Discuss communication overhead and Amdahl's Law
    \item \textbf{"Shared memory means no synchronization needed"}: Explain race conditions and critical sections
    \item \textbf{"Message passing is always slower"}: Discuss scalability benefits of distributed memory
    \item \textbf{"The game perfectly simulates HPC"}: Clarify simplifications and abstractions
\end{enumerate}

%----------------------------------------------------------------------------------------
